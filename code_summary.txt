Имя файла: docker-compose.yml
Путь к файлу: /home/user/megnet-deployment/docker-compose.yml
Содержимое файла:
version: "3.8"

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./src:/app/src # код приложения
      - ./src/models/meganet_model_best.pth:/app/src/models/meganet_model_best.pth
      - ./src/models/MegaNet_model_classes_1_3_vs_4_5.pth:/app/src/models/MegaNet_model_classes_1_3_vs_4_5.pth
      - ./src/models/MegaNet_model_classes_1_2_3.pth:/app/src/models/MegaNet_model_classes_1_2_3.pth
      - ./src/models/MegaNet_model_classes_4_5.pth:/app/src/models/MegaNet_model_classes_4_5.pth
    ports:
      - "8080:8080"
    environment:
      - PYTHONUNBUFFERED=1
    command: uvicorn src.server.api:app --host 0.0.0.0 --port 8080 --reload
--------------------------------------------------------------------------------
Имя файла: Dockerfile
Путь к файлу: /home/user/megnet-deployment/Dockerfile
Содержимое файла:
FROM python:3.10-slim

# Устанавливаем зависимости ОС
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Копируем зависимости и устанавливаем
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Копируем остальной код
COPY src/ ./src/

EXPOSE 8080

# Запуск FastAPI
CMD ["uvicorn", "src.server.api:app", "--host", "0.0.0.0", "--port", "8080"]
--------------------------------------------------------------------------------
Имя файла: megnet.py
Путь к файлу: /home/user/megnet-deployment/src/models/megnet.py
Содержимое файла:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# =============================================================================
# Вспомогательные классы для MegaNet
# =============================================================================
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadSelfAttention, self).__init__()
        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.layer_norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_output, _ = self.multihead_attn(x, x, x)
        return self.layer_norm(x + attn_output)

class ResidualBlock(nn.Module):
    def __init__(self, dim, dropout=0.1):
        super(ResidualBlock, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(dim, dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 2, dim),
            nn.Dropout(dropout)
        )
        self.layer_norm = nn.LayerNorm(dim)

    def forward(self, x):
        return self.layer_norm(x + self.layers(x))

# =============================================================================
# Основной класс модели MegaNet
# =============================================================================
class MegaNet(nn.Module):
    def __init__(self, input_size, num_classes, params):
        super(MegaNet, self).__init__()
        shared_embed_dim = params['shared_embed_dim']
        latent_dim = params['latent_dim']
        num_heads = params['num_heads']
        dropout = params['dropout']
        self.num_classes = num_classes

        # 1) Общий входной эмбеддинг
        self.shared_input_embed = nn.Sequential(
            nn.Linear(input_size, shared_embed_dim),
            nn.LayerNorm(shared_embed_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # 2) Трансформерная ветка
        self.trans_path = nn.Sequential(
            MultiHeadSelfAttention(shared_embed_dim, num_heads),
            ResidualBlock(shared_embed_dim, dropout)
        )

        # 3) VAE ветка
        self.vae_encoder_mu = nn.Linear(shared_embed_dim, latent_dim)
        self.vae_encoder_logvar = nn.Linear(shared_embed_dim, latent_dim)
        self.vae_decoder = nn.Linear(latent_dim, input_size)
        self.vae_transformer = nn.TransformerEncoderLayer(
            d_model=latent_dim,
            nhead=max(1, num_heads // 2),
            batch_first=True
        )

        # 4) Квантовая ветка
        num_states = 4
        self.quan_superposition = nn.Linear(shared_embed_dim, shared_embed_dim * num_states)
        self.quan_entanglement = nn.TransformerEncoderLayer(
            d_model=shared_embed_dim,
            nhead=max(1, num_heads // 2),
            batch_first=True
        )
        self.quan_measurement = nn.Linear(shared_embed_dim * num_states, shared_embed_dim)

        # 5) Эволюционные ветки
        self.evo_path1 = MultiHeadSelfAttention(shared_embed_dim, max(1, num_heads // 2))
        self.evo_path2 = ResidualBlock(shared_embed_dim)
        self.evo_gates = nn.Parameter(torch.ones(2))

        # 6) Диффузионная ветка
        self.diffu_path = nn.Sequential(
            ResidualBlock(shared_embed_dim, dropout),
            ResidualBlock(shared_embed_dim, dropout)
        )

        # 7) Слой объединения признаков и классификатор
        fusion_input_dim = shared_embed_dim + latent_dim + shared_embed_dim + shared_embed_dim + shared_embed_dim
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_input_dim, fusion_input_dim // 2),
            nn.BatchNorm1d(fusion_input_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        
        # Для бинарной классификации используем 1 выходной нейрон
        output_dim = 1 if num_classes == 2 else num_classes
        self.classifier = nn.Linear(fusion_input_dim // 2, output_dim)

        # Уровни шума (только для train)
        self.noise_levels = [0.05, 0.15]

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x_original):
        if self.training:
            noise = torch.randn_like(x_original) * np.random.choice(self.noise_levels)
            x = self.shared_input_embed(x_original + noise)
        else:
            x = self.shared_input_embed(x_original)

        # Transformer path
        out1 = self.trans_path(x.unsqueeze(1)).squeeze(1)

        # VAE path
        mu = self.vae_encoder_mu(x)
        logvar = self.vae_encoder_logvar(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.vae_decoder(z)
        out2 = self.vae_transformer(z.unsqueeze(1)).squeeze(1)

        # Quantum path
        b, _ = x.shape
        quan_states = self.quan_superposition(x).view(b, 4, -1)
        quan_entangled = self.quan_entanglement(quan_states)
        out3 = self.quan_measurement(quan_entangled.view(b, -1))

        # Evolution path
        g = F.softmax(self.evo_gates, dim=0)
        evo_out1 = self.evo_path1(x.unsqueeze(1)).squeeze(1)
        evo_out2 = self.evo_path2(x)
        out4 = g[0] * evo_out1 + g[1] * evo_out2

        # Diffusion path
        out5 = self.diffu_path(x)

        # Fusion
        concatenated = torch.cat([out1, out2, out3, out4, out5], dim=1)
        fused = self.fusion_layer(concatenated)
        logits = self.classifier(fused)
        
        # Возвращаем правильную размерность для бинарной классификации
        if self.num_classes == 2:
            logits = logits.squeeze(-1)
        
        return logits, recon_x, mu, logvar

# =============================================================================
# Обёртка для детерминированного инференса
# =============================================================================
class MegaNetInference(nn.Module):
    def __init__(self, base_model: MegaNet):
        super(MegaNetInference, self).__init__()
        self.base = base_model
        self.base.eval()
        # Отключаем все Dropout
        for m in self.base.modules():
            if isinstance(m, nn.Dropout):
                m.p = 0.0
        # Заменяем sampling на прямое mu
        self.base.reparameterize = lambda mu, logvar: mu

    def forward(self, x):
        x_emb = self.base.shared_input_embed(x)
        out1 = self.base.trans_path(x_emb.unsqueeze(1)).squeeze(1)
        mu = self.base.vae_encoder_mu(x_emb)
        out2 = self.base.vae_transformer(mu.unsqueeze(1)).squeeze(1)

        b, _ = x_emb.shape
        quan_states = self.base.quan_superposition(x_emb).view(b, 4, -1)
        quan_entangled = self.base.quan_entanglement(quan_states)
        out3 = self.base.quan_measurement(quan_entangled.view(b, -1))

        g = F.softmax(self.base.evo_gates, dim=0)
        evo_out1 = self.base.evo_path1(x_emb.unsqueeze(1)).squeeze(1)
        evo_out2 = self.base.evo_path2(x_emb)
        out4 = g[0] * evo_out1 + g[1] * evo_out2

        out5 = self.base.diffu_path(x_emb)
        concatenated = torch.cat([out1, out2, out3, out4, out5], dim=1)
        fused = self.base.fusion_layer(concatenated)
        logits = self.base.classifier(fused)
        return logits
--------------------------------------------------------------------------------
Имя файла: api.py
Путь к файлу: /home/user/megnet-deployment/src/server/api.py
Содержимое файла:
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler

# Импорт модели и device
from src.models.megnet import MegaNet

app = FastAPI(title="MegaNet Inference API")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- Настройка и загрузка ансамбля моделей ---
# Предопределенные гиперпараметры для ансамбля MegaNet
model_params = {
    'binary_1_3_vs_4_5': {'lr': 0.003261994572554154, 'dropout': 0.20634953989793595, 'shared_embed_dim': 128, 'latent_dim': 128, 'num_heads': 8, 'recon_weight': 0.13003271742797662, 'kl_weight': 0.007706404963519681, 'batch_size': 128},
    'multiclass_1_2_3': {'lr': 0.0001652456054246614, 'dropout': 0.1766769260535775, 'shared_embed_dim': 256, 'latent_dim': 128, 'num_heads': 4, 'recon_weight': 0.26985140479536573, 'kl_weight': 0.00038882113500409305, 'batch_size': 64},
    'binary_4_5': {'lr': 0.00018725722022078217, 'dropout': 0.2417088813617859, 'shared_embed_dim': 64, 'latent_dim': 128, 'num_heads': 8, 'recon_weight': 0.053753726714484695, 'kl_weight': 0.003242457328313236, 'batch_size': 64}
}

# Пути к сохраненным моделям ансамбля
model_paths = {
    'binary_1_3_vs_4_5': 'src/models/MegaNet_model_classes_1_3_vs_4_5.pth',
    'multiclass_1_2_3': 'src/models/MegaNet_model_classes_1_2_3.pth',
    'binary_4_5': 'src/models/MegaNet_model_classes_4_5.pth'
}

def load_meganet_model(model_path, input_size, num_classes, params, device):
    """Загружает обученную модель MegaNet"""
    model = MegaNet(input_size=input_size, num_classes=num_classes, params=params).to(device)
    
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # Обработка разных форматов сохранения модели
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        scaler = checkpoint['scaler']
    elif 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'])
        scaler = checkpoint['scaler']
    else:
        # Если это просто state_dict без обертки
        if 'scaler' in checkpoint:
            scaler = checkpoint['scaler']
            # Удаляем scaler из checkpoint и загружаем остальное как state_dict
            model_state = {k: v for k, v in checkpoint.items() if k != 'scaler'}
            model.load_state_dict(model_state)
        else:
            # Если это только веса модели без скейлера
            model.load_state_dict(checkpoint)
            # Создаем dummy скейлер
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            scaler.mean_ = np.zeros(input_size)
            scaler.scale_ = np.ones(input_size)
    
    model.eval()
    return model, scaler

# Параметры для каждой модели ансамбля
ensemble_configs = {
    'binary_1_3_vs_4_5': {'input_size': 8, 'num_classes': 2},
    'multiclass_1_2_3': {'input_size': 8, 'num_classes': 3},
    'binary_4_5': {'input_size': 8, 'num_classes': 2}
}

# Загрузка ансамбля моделей
print("Загрузка ансамбля моделей...")
best_params_1_3_vs_4_5 = model_params['binary_1_3_vs_4_5']
model_1_3_vs_4_5, scaler_1_3_vs_4_5 = load_meganet_model(
    model_paths['binary_1_3_vs_4_5'], 
    ensemble_configs['binary_1_3_vs_4_5']['input_size'], 
    ensemble_configs['binary_1_3_vs_4_5']['num_classes'], 
    best_params_1_3_vs_4_5, 
    device
)

best_params_1_2_3 = model_params['multiclass_1_2_3']
model_1_2_3, scaler_1_2_3 = load_meganet_model(
    model_paths['multiclass_1_2_3'], 
    ensemble_configs['multiclass_1_2_3']['input_size'], 
    ensemble_configs['multiclass_1_2_3']['num_classes'], 
    best_params_1_2_3, 
    device
)

best_params_4_5 = model_params['binary_4_5']
model_4_5, scaler_4_5 = load_meganet_model(
    model_paths['binary_4_5'], 
    ensemble_configs['binary_4_5']['input_size'], 
    ensemble_configs['binary_4_5']['num_classes'], 
    best_params_4_5, 
    device
)

# Обертка для ансамбля моделей
class MegaNetEnsembleInference:
    def __init__(self, model_1_3_vs_4_5, scaler_1_3_vs_4_5, model_1_2_3, scaler_1_2_3, model_4_5, scaler_4_5, device):
        self.model_1_3_vs_4_5 = model_1_3_vs_4_5
        self.scaler_1_3_vs_4_5 = scaler_1_3_vs_4_5
        self.model_1_2_3 = model_1_2_3
        self.scaler_1_2_3 = scaler_1_2_3
        self.model_4_5 = model_4_5
        self.scaler_4_5 = scaler_4_5
        self.device = device
        
        # Переводим все модели в режим inference
        for model in [self.model_1_3_vs_4_5, self.model_1_2_3, self.model_4_5]:
            model.eval()
            # Отключаем dropout
            for m in model.modules():
                if isinstance(m, torch.nn.Dropout):
                    m.p = 0.0
    
    def predict(self, X):
        # Масштабирование данных для первой модели
        X_scaled_1_3_vs_4_5 = self.scaler_1_3_vs_4_5.transform(X.reshape(1, -1))
        X_tensor_1_3_vs_4_5 = torch.FloatTensor(X_scaled_1_3_vs_4_5).to(self.device)
        
        # Предсказание первой модели (1-3 vs 4-5)
        with torch.no_grad():
            outputs_1_3_vs_4_5, _, _, _ = self.model_1_3_vs_4_5(X_tensor_1_3_vs_4_5)
            probs_1_3_vs_4_5 = torch.sigmoid(outputs_1_3_vs_4_5).cpu().numpy()
            initial_pred = (probs_1_3_vs_4_5 > 0.5).astype(int)[0]
        
        # Если предсказание 0 (классы 1-3)
        if initial_pred == 0:
            X_scaled_1_2_3 = self.scaler_1_2_3.transform(X.reshape(1, -1))
            X_tensor_1_2_3 = torch.FloatTensor(X_scaled_1_2_3).to(self.device)
            
            with torch.no_grad():
                outputs_1_2_3, _, _, _ = self.model_1_2_3(X_tensor_1_2_3)
                pred_1_2_3 = torch.argmax(outputs_1_2_3, dim=1).cpu().numpy()[0]
                final_pred = pred_1_2_3 + 1  # Сдвиг обратно к 1, 2, 3
        
        # Если предсказание 1 (классы 4-5)
        else:
            X_scaled_4_5 = self.scaler_4_5.transform(X.reshape(1, -1))
            X_tensor_4_5 = torch.FloatTensor(X_scaled_4_5).to(self.device)
            
            with torch.no_grad():
                outputs_4_5, _, _, _ = self.model_4_5(X_tensor_4_5)
                probs_4_5_raw = torch.sigmoid(outputs_4_5).cpu().numpy()
                pred_4_5 = (probs_4_5_raw > 0.5).astype(int)[0]
                final_pred = pred_4_5 + 4  # Сдвиг к 4 или 5
        
        return final_pred

# Создаем экземпляр ансамбля
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
ensemble_model = MegaNetEnsembleInference(
    model_1_3_vs_4_5, scaler_1_3_vs_4_5,
    model_1_2_3, scaler_1_2_3,
    model_4_5, scaler_4_5,
    device
)

# --- Схемы Pydantic ---
class Inp(BaseModel):
    values: list[float]  # длина списка должна быть = input_size

class EnsemblePrediction(BaseModel):
    predicted_class: int

# --- Эндпоинт /predict_ensemble для ансамбля моделей ---
@app.post("/predict_ensemble", response_model=EnsemblePrediction)
async def predict_ensemble(inp: Inp):
    if len(inp.values) != 8:
        raise HTTPException(status_code=400,
                          detail=f"Input vector must have length 8")
    
    # Преобразуем в numpy array
    x_raw = np.array(inp.values)
    
    # Используем ансамбль для предсказания
    predicted_class = ensemble_model.predict(x_raw)
    
    return EnsemblePrediction(predicted_class=int(predicted_class))
--------------------------------------------------------------------------------
Имя файла: requirements.txt
Путь к файлу: /home/user/megnet-deployment/requirements.txt
Содержимое файла:
annotated-types==0.7.0
anyio==4.9.0
certifi==2025.8.3
charset-normalizer==3.4.2
click==8.2.1
exceptiongroup==1.3.0
fastapi==0.116.1
filelock==3.18.0
fsspec==2025.7.0
h11==0.16.0
httptools==0.6.4
idna==3.10
iniconfig==2.1.0
Jinja2==3.1.6
joblib==1.5.1
MarkupSafe==3.0.2
mpmath==1.3.0
networkx==3.4.2
numpy==2.2.6
nvidia-cublas-cu12==12.6.4.1
nvidia-cuda-cupti-cu12==12.6.80
nvidia-cuda-nvrtc-cu12==12.6.77
nvidia-cuda-runtime-cu12==12.6.77
nvidia-cudnn-cu12==9.5.1.17
nvidia-cufft-cu12==11.3.0.4
nvidia-cufile-cu12==1.11.1.6
nvidia-curand-cu12==10.3.7.77
nvidia-cusolver-cu12==11.7.1.2
nvidia-cusparse-cu12==12.5.4.2
nvidia-cusparselt-cu12==0.6.3
nvidia-nccl-cu12==2.26.2
nvidia-nvjitlink-cu12==12.6.85
nvidia-nvtx-cu12==12.6.77
packaging==25.0
pandas==2.3.1
pluggy==1.6.0
prometheus-fastapi-instrumentator==7.1.0
prometheus_client==0.22.1
pydantic==2.11.7
pydantic_core==2.33.2
Pygments==2.19.2
pytest==8.4.1
python-dateutil==2.9.0.post0
python-dotenv==1.1.1
pytz==2025.2
PyYAML==6.0.2
requests==2.32.4
scikit-learn==1.6.1
scipy==1.15.3
six==1.17.0
sniffio==1.3.1
starlette==0.47.2
sympy==1.14.0
threadpoolctl==3.6.0
tomli==2.2.1
torch==2.7.1
triton==3.3.1
typing-inspection==0.4.1
typing_extensions==4.14.1
tzdata==2025.2
urllib3==2.5.0
uvicorn==0.35.0
uvloop==0.21.0
watchfiles==1.1.0
websockets==15.0.1

--------------------------------------------------------------------------------
